{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OllamaPhi' from 'langchain' (C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OllamaPhi\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#from langchain_community.chat_models import ChatOllama\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#from langchain import Ollama\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#ollama = ChatOllama(model=\"llama2\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'OllamaPhi' from 'langchain' (C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "from langchain import OllamaPhi\n",
    "#from langchain_community.chat_models import ChatOllama\n",
    "#from langchain import Ollama\n",
    "#ollama = ChatOllama(model=\"llama2\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize Faiss index\n",
    "d = 768  # Dimension of embeddings (adjust based on your embedding model)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Initialize Ollama Phi module\n",
    "#llama2 = llama2()\n",
    "\n",
    "\n",
    "ollama_phi = ollamaPhi()\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_file():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file part'}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "    try:\n",
    "        # Extract text from PDF\n",
    "        pdf_text = extract_text_from_pdf(file)\n",
    "        # Divide PDF text into chunks (e.g., paragraphs)\n",
    "        chunks = split_into_chunks(pdf_text)\n",
    "        # Embed each chunk\n",
    "        embeddings = []\n",
    "        for chunk in chunks:\n",
    "            # Compute embeddings (dummy code)\n",
    "            embedding = compute_embedding(chunk)\n",
    "            embeddings.append(embedding)\n",
    "        # Convert embeddings to Faiss-compatible format\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        # Add embeddings to Faiss index\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        return jsonify({'message': 'File uploaded and processed successfully'}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    # Function to extract text from PDF using PyMuPDF (fitz)\n",
    "    doc = fitz.open(file)\n",
    "    pdf_text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        pdf_text += page.get_text()\n",
    "    doc.close()\n",
    "    return pdf_text\n",
    "\n",
    "def split_into_chunks(text):\n",
    "    # Dummy function to split text into chunks (e.g., paragraphs)\n",
    "    return text.split('\\n\\n')  # Split by double newline for paragraphs\n",
    "\n",
    "def compute_embedding(text):\n",
    "    # Dummy function to compute embeddings (replace with actual embedding computation)\n",
    "    return [0.0] * d  # Return a vector of zeros (dummy)\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask_question():\n",
    "    question = request.json.get('question', '')\n",
    "    if not question:\n",
    "        return jsonify({'error': 'Question not provided'}), 400\n",
    "\n",
    "    try:\n",
    "        # Query Faiss index to retrieve relevant embeddings\n",
    "        # Dummy code to retrieve embeddings (use actual querying mechanism)\n",
    "        query_embedding = compute_embedding(question)\n",
    "        D, I = index.search(query_embedding, k=5)  # Retrieve top 5 closest embeddings\n",
    "        \n",
    "        # Get corresponding chunks from the PDF (dummy code)\n",
    "        answers = []\n",
    "        for idx in I[0]:\n",
    "            answers.append(chunks[idx])  # Assuming 'chunks' is a list of text chunks\n",
    "\n",
    "        # Use Ollama Phi for question answering (dummy code)\n",
    "        response = ollama_phi.answer_question(question, answers)\n",
    "       # response = llama2.answer_question(question, answers)\n",
    "\n",
    "        return jsonify({'answer': response}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "# pip install --upgrade langchain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
